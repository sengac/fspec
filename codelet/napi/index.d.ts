/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

/** Token usage tracking information */
export interface TokenTracker {
  inputTokens: number;
  outputTokens: number;
  cacheReadInputTokens?: number;
  cacheCreationInputTokens?: number;
}
/** Tool call information */
export interface ToolCallInfo {
  id: string;
  name: string;
  input: string;
}
/** Tool result information */
export interface ToolResultInfo {
  toolCallId: string;
  content: string;
  isError: boolean;
}
/** Stream chunk types for streaming responses */
export const enum ChunkType {
  Text = 'Text',
  ToolCall = 'ToolCall',
  ToolResult = 'ToolResult',
  Done = 'Done',
  Error = 'Error',
}
/** A chunk of streaming response */
export interface StreamChunk {
  type: string;
  text?: string;
  toolCall?: ToolCallInfo;
  toolResult?: ToolResultInfo;
  error?: string;
}
/** Message role enum */
export const enum MessageRole {
  System = 'System',
  User = 'User',
  Assistant = 'Assistant',
}
/** A conversation message (simplified for JS) */
export interface Message {
  role: string;
  content: string;
}
/**
 * CodeletSession - Main class for AI agent interactions
 *
 * Exposes codelet's Rust AI agent functionality to Node.js.
 */
export declare class CodeletSession {
  /**
   * Create a new CodeletSession
   *
   * If provider_name is not specified, auto-detects the highest priority available provider.
   * Priority order: Claude > Gemini > Codex > OpenAI
   */
  constructor(providerName?: string | undefined | null);
  /** Get the current provider name */
  get currentProviderName(): string;
  /** Get list of available providers */
  get availableProviders(): Array<string>;
  /** Get the token usage tracker */
  get tokenTracker(): TokenTracker;
  /** Get conversation messages (simplified representation) */
  get messages(): Array<Message>;
  /** Switch to a different provider */
  switchProvider(providerName: string): Promise<void>;
  /** Clear conversation history */
  clearHistory(): void;
  /**
   * Send a prompt and stream the response
   *
   * The callback receives StreamChunk objects with type: 'text', 'tool_call', 'tool_result', 'done', or 'error'
   */
  prompt(input: string, callback: (chunk: StreamChunk) => void): Promise<void>;
}
