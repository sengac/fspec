/* auto-generated by NAPI-RS */
/* eslint-disable */
/**
 * CodeletSession - Main class for AI agent interactions
 *
 * Exposes codelet's Rust AI agent functionality to Node.js.
 */
export declare class CodeletSession {
  /**
   * Create a new CodeletSession
   *
   * If provider_name is not specified, auto-detects the highest priority available provider.
   * Priority order: Claude > Gemini > Codex > OpenAI
   */
  constructor(providerName?: string | undefined | null);
  /**
   * MODEL-001: Create a new CodeletSession with dynamic model selection
   *
   * Creates a session with model support enabled, allowing selection of specific
   * models from models.dev. The model string should be in "provider/model-id" format.
   *
   * # Arguments
   * * `model_string` - Model in "provider/model-id" format (e.g., "anthropic/claude-sonnet-4")
   *
   * # Example
   * ```typescript
   * const session = await CodeletSession.newWithModel("anthropic/claude-sonnet-4");
   * ```
   */
  static newWithModel(modelString: string): Promise<CodeletSession>;
  /**
   * Interrupt the current agent execution
   *
   * Call this when the user presses Esc in the TUI.
   * The agent will stop immediately via tokio::sync::Notify (NAPI-004).
   * The notify_one() call wakes the tokio::select! in stream_loop,
   * allowing immediate response to ESC even during blocking operations.
   *
   * IMPORTANT: Uses notify_one() instead of notify_waiters() because:
   * - notify_waiters() only wakes CURRENTLY waiting tasks (notification lost if none waiting)
   * - notify_one() stores a permit if no one waiting, so next notified() returns immediately
   * This eliminates the race condition between flag check and entering tokio::select!
   */
  interrupt(): void;
  /**
   * Reset the interrupt flag
   *
   * Called automatically at the start of each prompt, but can be called
   * manually if needed.
   */
  resetInterrupt(): void;
  /**
   * Toggle debug capture mode (AGENT-021)
   *
   * Mirrors CLI repl_loop.rs:36-67 logic.
   * When enabling, sets session metadata (provider, model, context_window).
   * When disabling, stops capture and returns path to saved session file.
   *
   * If debug_dir is provided, debug files will be written to `{debug_dir}/debug/`
   * instead of the default directory. For fspec, pass `~/.fspec` to write to
   * `~/.fspec/debug/`.
   */
  toggleDebug(debugDir?: string | undefined | null): DebugCommandResult;
  /**
   * Manually trigger context compaction (NAPI-005)
   *
   * Mirrors CLI repl_loop.rs /compact command logic.
   * Calls execute_compaction from interactive_helpers to compress context.
   *
   * Returns CompactionResult with metrics about the compaction operation.
   * Returns error if session is empty (nothing to compact).
   */
  compact(): Promise<CompactionResult>;
  /** Get the current provider name */
  get currentProviderName(): string;
  /**
   * MODEL-001: Get the currently selected model string
   *
   * Returns the model string in "provider/model-id" format (e.g., "anthropic/claude-sonnet-4")
   * if a model was explicitly selected via newWithModel() or selectModel(), otherwise None.
   */
  get selectedModel(): string | null;
  /** Get list of available providers */
  get availableProviders(): Array<string>;
  /** Get the token usage tracker */
  get tokenTracker(): TokenTracker;
  /** Get conversation messages (simplified representation) */
  get messages(): Array<Message>;
  /** Switch to a different provider */
  switchProvider(providerName: string): Promise<void>;
  /**
   * MODEL-001: Select a different model mid-session
   *
   * Changes the model used for subsequent prompts without clearing conversation history.
   * The model string should be in "provider/model-id" format.
   *
   * NOTE: This only works for sessions created via newWithModel(). Sessions created
   * via the default constructor do not have model registry support enabled.
   *
   * # Arguments
   * * `model_string` - Model in "provider/model-id" format (e.g., "anthropic/claude-sonnet-4")
   *
   * # Example
   * ```typescript
   * const session = await CodeletSession.newWithModel("anthropic/claude-sonnet-4");
   * // ... use session ...
   * await session.selectModel("anthropic/claude-opus-4");  // Switch to Opus
   * ```
   */
  selectModel(modelString: string): Promise<void>;
  /**
   * Clear conversation history and reinject context reminders
   *
   * Clears messages, turns, and token tracker, then reinjects context reminders
   * (CLAUDE.md discovery, environment info) to maintain project context.
   *
   * CRITICAL (AGENT-003): Must call inject_context_reminders() after clearing
   * to restore project context (CLAUDE.md, environment info). Without this,
   * the AI loses CLAUDE.md context on the next prompt after /clear.
   */
  clearHistory(): void;
  /**
   * Restore messages from a persisted session (NAPI-003)
   *
   * Restores conversation history from persistence into the CodeletSession's
   * internal message array, enabling the LLM to have context of the restored
   * conversation.
   *
   * CRITICAL: This method must be called when resuming a session to ensure
   * the AI has context of the previous conversation. Without this, the AI
   * would start fresh despite the UI showing historical messages.
   *
   * # Arguments
   * * `messages` - Array of messages to restore (from persistenceGetSessionMessages)
   *
   * # Process
   * 1. Clears existing messages, turns, and token tracker
   * 2. Converts each persistence message to rig::message::Message format
   * 3. Injects context reminders (CLAUDE.md, environment info)
   * 4. Messages are ready for use in next prompt
   */
  restoreMessages(messages: Array<Message>): void;
  /**
   * Restore messages from full envelope JSON strings (NAPI-008)
   *
   * This is the preferred method for restoring sessions as it preserves:
   * - Structured tool_use and tool_result blocks (not just text summaries)
   * - Multi-part message content (text + tool calls)
   * - Turn boundaries for compaction (rebuilt via convert_messages_to_turns)
   *
   * Call restoreTokenState() after this to restore token counts.
   *
   * # Arguments
   * * `envelopes` - Array of envelope JSON strings from persistenceGetSessionMessageEnvelopes
   */
  restoreMessagesFromEnvelopes(envelopes: Array<string>): void;
  /**
   * Restore token state from persisted session (TUI-033, NAPI-008)
   *
   * Call this after restoreMessages() or restoreMessagesFromEnvelopes() to restore
   * the token counts from the persisted session manifest. This ensures getContextFillInfo()
   * returns accurate context fill percentage after session restoration.
   *
   * # Arguments
   * * `input_tokens` - Total input tokens from session manifest
   * * `output_tokens` - Total output tokens from session manifest
   * * `cache_read_tokens` - Cache read tokens from session manifest
   * * `cache_creation_tokens` - Cache creation tokens from session manifest
   */
  restoreTokenState(
    inputTokens: number,
    outputTokens: number,
    cacheReadTokens: number,
    cacheCreationTokens: number,
    cumulativeBilledInput: number,
    cumulativeBilledOutput: number
  ): void;
  /**
   * Get current context fill info (TUI-033)
   *
   * Returns the current context fill percentage and related metrics.
   * Call this after restoreMessages() to get initial context fill state,
   * since restoring messages doesn't trigger streaming events.
   *
   * # Returns
   * * `ContextFillInfo` with fill_percentage, effective_tokens, threshold, context_window
   */
  getContextFillInfo(): ContextFillInfo;
  /**
   * Send a prompt and stream the response
   *
   * The callback receives StreamChunk objects with type: 'Text', 'Thinking', 'ToolCall', 'ToolResult', 'Done', or 'Error'
   *
   * Uses the same streaming infrastructure as codelet-cli:
   * - run_agent_stream for shared streaming logic
   * - StreamOutput trait for polymorphic output
   * - is_interrupted flag for Esc key handling (set via interrupt() method)
   *
   * # Arguments
   * * `input` - The user prompt text
   * * `thinking_config` - Optional JSON string from getThinkingConfig() (TOOL-010)
   * * `callback` - Stream callback for receiving chunks
   */
  prompt(
    input: string,
    thinkingConfig: string | undefined | null,
    callback: (chunk: StreamChunk) => void
  ): Promise<void>;
}

/** Stream chunk types for streaming responses (TOOL-010) */
export declare const enum ChunkType {
  Text = 'Text',
  /** Thinking/reasoning content from extended thinking (TOOL-010) */
  Thinking = 'Thinking',
  ToolCall = 'ToolCall',
  ToolResult = 'ToolResult',
  /** Tool execution progress - streaming output from bash/shell tools (TOOL-011) */
  ToolProgress = 'ToolProgress',
  Status = 'Status',
  Interrupted = 'Interrupted',
  TokenUpdate = 'TokenUpdate',
  ContextFillUpdate = 'ContextFillUpdate',
  Done = 'Done',
  Error = 'Error',
}

/**
 * Compaction result (NAPI-005)
 * Returned by compact() with metrics about the compaction operation
 */
export interface CompactionResult {
  /** Original token count before compaction */
  originalTokens: number;
  /** Token count after compaction */
  compactedTokens: number;
  /** Compression ratio as percentage (0-100) */
  compressionRatio: number;
  /** Number of turns summarized */
  turnsSummarized: number;
  /** Number of turns kept */
  turnsKept: number;
}

/**
 * Context window fill information (TUI-033)
 * Sent with each token update to show context window usage
 */
export interface ContextFillInfo {
  /** Fill percentage (0-100+, can exceed 100 near compaction) */
  fillPercentage: number;
  /** Effective tokens (after cache discount) - using f64 for NAPI compatibility */
  effectiveTokens: number;
  /** Compaction threshold (usable context after output reservation) - using f64 for NAPI compatibility */
  threshold: number;
  /** Provider's context window size - using f64 for NAPI compatibility */
  contextWindow: number;
}

/**
 * Debug command result (AGENT-021)
 * Returned by toggleDebug() to indicate debug capture state
 */
export interface DebugCommandResult {
  /** Whether debug capture is now enabled */
  enabled: boolean;
  /** Path to the debug session file (if available) */
  sessionFile?: string;
  /** Human-readable message about the result */
  message: string;
}

/**
 * Extract thinking text from a response part.
 *
 * # Arguments
 * * `provider` - Provider identifier: "gemini-3", "gemini-2.5", "claude", etc.
 * * `part_json` - JSON string of the response part
 *
 * # Returns
 * The thinking text if present, null otherwise.
 */
export declare function extractThinkingText(
  provider: string,
  partJson: string
): string | null;

/**
 * Get thinking configuration JSON for a provider at a specific level.
 *
 * # Arguments
 * * `provider` - Provider identifier: "gemini-3", "gemini-2.5", "claude", etc.
 * * `level` - Thinking intensity level
 *
 * # Returns
 * JSON string containing the provider-specific thinking configuration.
 *
 * # Example
 * ```typescript
 * import { getThinkingConfig, JsThinkingLevel } from '@anthropic/codelet-napi';
 *
 * const config = JSON.parse(getThinkingConfig('gemini-3', JsThinkingLevel.High));
 * // { thinkingConfig: { includeThoughts: true, thinkingLevel: "high" } }
 * ```
 */
export declare function getThinkingConfig(
  provider: string,
  level: JsThinkingLevel
): string;

/**
 * Check if a response part contains thinking content.
 *
 * # Arguments
 * * `provider` - Provider identifier: "gemini-3", "gemini-2.5", "claude", etc.
 * * `part_json` - JSON string of the response part
 *
 * # Returns
 * true if the part contains thinking/reasoning content, false otherwise.
 *
 * # Example
 * ```typescript
 * import { isThinkingContent } from '@anthropic/codelet-napi';
 *
 * const part = JSON.stringify({ thought: true, text: "Let me think..." });
 * const isThinking = isThinkingContent('gemini-3', part);
 * // true
 * ```
 */
export declare function isThinkingContent(
  provider: string,
  partJson: string
): boolean;

/** TypeScript-friendly thinking level enum */
export declare const enum JsThinkingLevel {
  /** Disable thinking/reasoning entirely */
  Off = 0,
  /** Minimal thinking (fast responses) */
  Low = 1,
  /** Balanced thinking (default for most tasks) */
  Medium = 2,
  /** Maximum thinking (complex reasoning tasks) */
  High = 3,
}

/** A conversation message (simplified for JS) */
export interface Message {
  role: string;
  content: string;
}

/** Message role enum */
export declare const enum MessageRole {
  System = 'System',
  User = 'User',
  Assistant = 'Assistant',
}

/**
 * Get the current cache directory for model data
 *
 * Returns the custom directory if set via modelsSetCacheDirectory(),
 * otherwise returns ~/.fspec/cache as the default.
 */
export declare function modelsGetCacheDirectory(): string;

/**
 * Get information for a specific model (async)
 *
 * # Arguments
 * * `provider_id` - Provider ID (e.g., "anthropic")
 * * `model_id` - Model ID (e.g., "claude-sonnet-4")
 */
export declare function modelsGetInfo(
  providerId: string,
  modelId: string
): Promise<NapiModelInfo>;

/**
 * List all available models from models.dev (async)
 *
 * Returns models grouped by provider. Uses cached registry for efficiency.
 * First call loads from disk/API, subsequent calls use cached data.
 *
 * Filters out:
 * - Deprecated models (status = "deprecated")
 * - Models older than 18 months
 *
 * Sorts models by release date (newest first).
 */
export declare function modelsListAll(): Promise<Array<NapiProviderModels>>;

/**
 * List models for a specific provider (async)
 *
 * # Arguments
 * * `provider_id` - Provider ID (e.g., "anthropic", "openai", "google")
 */
export declare function modelsListForProvider(
  providerId: string
): Promise<Array<NapiModelInfo>>;

/**
 * Refresh the model cache from models.dev API (async)
 *
 * Forces a fresh fetch from the API, ignoring cached data.
 * NOTE: This does NOT invalidate the in-memory registry cache.
 * For a full refresh, restart the process after calling this.
 *
 * Returns the number of providers loaded.
 */
export declare function modelsRefreshCache(): Promise<number>;

/**
 * Set the cache directory for model data (e.g., ~/.fspec/cache)
 *
 * IMPORTANT: This MUST be called BEFORE any other model operations.
 * The directory setting is captured when the registry is first loaded.
 * Calling this after other model functions will have no effect until
 * modelsRefreshCache() is called.
 *
 * # Arguments
 * * `dir` - The directory path for cache data (models.json will be stored here)
 */
export declare function modelsSetCacheDirectory(dir: string): void;

export interface NapiAppendResult {
  messageId: string;
  session: NapiSessionManifest;
}

export interface NapiCherryPickResult {
  session: NapiSessionManifest;
  importedIndices: Array<number>;
}

export interface NapiCompactionState {
  summary: string;
  compactedBeforeIndex: number;
  compactedAt: string;
}

export interface NapiForkPoint {
  sourceSessionId: string;
  forkAfterIndex: number;
  forkedAt: string;
}

export interface NapiHistoryEntry {
  display: string;
  timestamp: string;
  project: string;
  sessionId: string;
  hasPastedContent: boolean;
}

export interface NapiMergeRecord {
  sourceSessionId: string;
  sourceIndices: Array<number>;
  insertedAt?: number;
  mergedAt: string;
}

/** Model information from models.dev */
export interface NapiModelInfo {
  /** The API model ID (e.g., "claude-sonnet-4-20250514") */
  id: string;
  /** Display name (e.g., "Claude Sonnet 4") */
  name: string;
  /** Model family (e.g., "claude-sonnet") */
  family?: string;
  /** Whether model supports reasoning/thinking */
  reasoning: boolean;
  /** Whether model supports tool calls */
  toolCall: boolean;
  /** Whether model supports file/image attachments */
  attachment: boolean;
  /** Whether model supports temperature parameter */
  temperature: boolean;
  /** Context window size in tokens */
  contextWindow: number;
  /** Maximum output tokens */
  maxOutput: number;
  /** Whether model has vision capability (image input) */
  hasVision: boolean;
}

/** Provider with its available models */
export interface NapiProviderModels {
  /** Provider ID (e.g., "anthropic", "openai", "google") */
  providerId: string;
  /** Provider display name (e.g., "Anthropic", "OpenAI", "Google") */
  providerName: string;
  /** List of models available from this provider */
  models: Array<NapiModelInfo>;
}

export interface NapiSessionManifest {
  id: string;
  name: string;
  project: string;
  provider: string;
  createdAt: string;
  updatedAt: string;
  messageCount: number;
  forkedFrom?: NapiForkPoint;
  mergedFrom: Array<NapiMergeRecord>;
  compaction?: NapiCompactionState;
  tokenUsage: NapiTokenUsage;
}

export interface NapiStoredMessage {
  id: string;
  contentHash: string;
  createdAt: string;
  role: string;
  content: string;
  tokenCount?: number;
  blobRefs: Array<string>;
  /** Metadata as a JSON string */
  metadataJson: string;
}

/**
 * Token usage with dual metrics (CTX-003)
 *
 * - `current_context_tokens`: Latest context size (for display and threshold checks)
 * - `cumulative_billed_input`: Sum of all API calls (for billing analytics)
 * - `cumulative_billed_output`: Sum of all API output tokens (for billing analytics)
 */
export interface NapiTokenUsage {
  /**
   * Current context size (latest input_tokens from API - overwritten, not accumulated)
   * CTX-003: This is what should be displayed to users and used for threshold checks
   */
  currentContextTokens: number;
  /**
   * Cumulative billed input tokens (sum of all API calls - for billing analytics)
   * CTX-003: This is the total billed by Anthropic across all API calls
   */
  cumulativeBilledInput: number;
  /** Cumulative billed output tokens (sum of all API calls) */
  cumulativeBilledOutput: number;
  /** Cache read tokens from current API call */
  cacheReadTokens: number;
  /** Cache creation tokens from current API call */
  cacheCreationTokens: number;
}

/** Add a history entry */
export declare function persistenceAddHistory(
  display: string,
  project: string,
  sessionId: string
): void;

/** Append a message to a session */
export declare function persistenceAppendMessage(
  sessionId: string,
  role: string,
  content: string
): NapiAppendResult;

/**
 * Append a message with metadata to a session
 *
 * metadata_json should be a JSON object string, e.g. '{"model": "claude-3", "stop_reason": "end_turn"}'
 */
export declare function persistenceAppendMessageWithMetadata(
  sessionId: string,
  role: string,
  content: string,
  metadataJson: string
): NapiAppendResult;

/** Check if a blob exists */
export declare function persistenceBlobExists(hash: string): boolean;

/** Cherry-pick messages with context */
export declare function persistenceCherryPick(
  targetId: string,
  sourceId: string,
  index: number,
  context: number
): NapiCherryPickResult;

/** Cleanup orphaned messages */
export declare function persistenceCleanupOrphanedMessages(): number;

/** Clear compaction state for a session */
export declare function persistenceClearCompactionState(
  sessionId: string
): NapiSessionManifest;

/** Create a new session */
export declare function persistenceCreateSession(
  name: string,
  project: string
): NapiSessionManifest;

/** Create a new session with a specific provider */
export declare function persistenceCreateSessionWithProvider(
  name: string,
  project: string,
  provider: string
): NapiSessionManifest;

/** Delete a session */
export declare function persistenceDeleteSession(id: string): void;

/** Fork a session at a specific message index */
export declare function persistenceForkSession(
  sessionId: string,
  atIndex: number,
  name: string
): NapiSessionManifest;

/** Get content from blob storage */
export declare function persistenceGetBlob(hash: string): Buffer;

/** Get the current data directory */
export declare function persistenceGetDataDirectory(): string;

/** Get history entries */
export declare function persistenceGetHistory(
  project?: string | undefined | null,
  limit?: number | undefined | null
): Array<NapiHistoryEntry>;

/** Get a message by ID */
export declare function persistenceGetMessage(
  id: string
): NapiStoredMessage | null;

/** Get a message as a full envelope JSON with blob content rehydrated */
export declare function persistenceGetMessageEnvelope(
  id: string
): string | null;

/**
 * Get a message envelope WITHOUT blob rehydration (returns blob references as-is)
 * Use this when you want to inspect the raw stored format with blob:sha256: references.
 */
export declare function persistenceGetMessageEnvelopeRaw(
  id: string
): string | null;

/** Get all messages for a session as envelope JSON array with blob content rehydrated */
export declare function persistenceGetSessionMessageEnvelopes(
  sessionId: string
): Array<string>;

/** Get all messages for a session WITHOUT blob rehydration (returns blob references as-is) */
export declare function persistenceGetSessionMessageEnvelopesRaw(
  sessionId: string
): Array<string>;

/** Get all messages for a session */
export declare function persistenceGetSessionMessages(
  sessionId: string
): Array<NapiStoredMessage>;

/** List all sessions for a project */
export declare function persistenceListSessions(
  project: string
): Array<NapiSessionManifest>;

/** Load a session by ID */
export declare function persistenceLoadSession(id: string): NapiSessionManifest;

/** Merge messages from another session */
export declare function persistenceMergeMessages(
  targetId: string,
  sourceId: string,
  indices: Array<number>
): NapiSessionManifest;

/** Rename a session */
export declare function persistenceRenameSession(
  id: string,
  newName: string
): void;

/** Resume the last session for a project */
export declare function persistenceResumeLastSession(
  project: string
): NapiSessionManifest;

/** Search history entries */
export declare function persistenceSearchHistory(
  query: string,
  project?: string | undefined | null
): Array<NapiHistoryEntry>;

/** Set compaction state for a session (after manual or automatic compaction) */
export declare function persistenceSetCompactionState(
  sessionId: string,
  summary: string,
  compactedBeforeIndex: number
): NapiSessionManifest;

/**
 * Set the data directory for persistence (e.g., ~/.fspec or ~/.codelet)
 *
 * This must be called before any other persistence operations if you want
 * to use a custom directory instead of the default ~/.fspec.
 */
export declare function persistenceSetDataDirectory(dir: string): void;

/** Set session token usage (REPLACES existing - use for cumulative totals) */
export declare function persistenceSetSessionTokens(
  sessionId: string,
  input: number,
  output: number,
  cacheRead: number,
  cacheCreate: number,
  cumulativeInput: number,
  cumulativeOutput: number
): NapiSessionManifest;

/** Store content in blob storage */
export declare function persistenceStoreBlob(content: Buffer): string;

/**
 * Store a message envelope as JSON
 *
 * This is the primary function for storing Claude Code format messages.
 * It handles blob storage for large content automatically.
 */
export declare function persistenceStoreMessageEnvelope(
  sessionId: string,
  envelopeJson: string
): NapiAppendResult;

/** Update session token usage (ADDS to existing) */
export declare function persistenceUpdateSessionTokens(
  sessionId: string,
  input: number,
  output: number,
  cacheRead: number,
  cacheCreate: number
): NapiSessionManifest;

/** Set the logging callback from TypeScript and initialize the tracing subscriber */
export declare function setRustLogCallback(callback: LogCallback): void;

/** A chunk of streaming response (TOOL-010: added thinking field, TOOL-011: added tool_progress) */
export interface StreamChunk {
  type: string;
  text?: string;
  /** Thinking/reasoning content from extended thinking (TOOL-010) */
  thinking?: string;
  toolCall?: ToolCallInfo;
  toolResult?: ToolResultInfo;
  /** Tool execution progress - streaming output from bash/shell tools (TOOL-011) */
  toolProgress?: ToolProgressInfo;
  status?: string;
  queuedInputs?: Array<string>;
  tokens?: TokenTracker;
  contextFill?: ContextFillInfo;
  error?: string;
}

/** Token usage tracking information */
export interface TokenTracker {
  inputTokens: number;
  outputTokens: number;
  cacheReadInputTokens?: number;
  cacheCreationInputTokens?: number;
  /** Tokens per second (EMA-smoothed, calculated in Rust) */
  tokensPerSecond?: number;
  /** Cumulative billed input tokens (sum of all API calls) */
  cumulativeBilledInput?: number;
  /** Cumulative billed output tokens (sum of all API calls) */
  cumulativeBilledOutput?: number;
}

/** Tool call information */
export interface ToolCallInfo {
  id: string;
  name: string;
  input: string;
}

/**
 * Tool execution progress information (TOOL-011)
 * Streaming output from bash/shell tools during execution
 */
export interface ToolProgressInfo {
  /** Tool call ID this progress is for */
  toolCallId: string;
  /** Tool name (e.g., "bash", "run_shell_command") */
  toolName: string;
  /** Output chunk (new text since last progress event) */
  outputChunk: string;
}

/** Tool result information */
export interface ToolResultInfo {
  toolCallId: string;
  content: string;
  isError: boolean;
}
