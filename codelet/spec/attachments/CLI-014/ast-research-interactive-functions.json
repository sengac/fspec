{
  "matches": [
    {
      "type": "function_item",
      "name": "estimate_tokens",
      "line": 35,
      "column": 0,
      "text": "fn estimate_tokens(text: &str) -> u64 {\n    (text.len() / APPROX_BYTES_PER_TOKEN) as u64\n}"
    },
    {
      "type": "function_item",
      "name": "run_interactive_mode",
      "line": 40,
      "column": 0,
      "text": "pub async fn run_interactive_mode(provider_name: Option<&str>) -> Result<()> {\n    // Initialize session with persistent context (CLI-008)\n    let mut session = Session::new(provider_name)?;\n\n    // Display startup card\n    display_startup_card(&session)?;\n\n    // Main REPL loop (raw mode is enabled/disabled per-request, not globally)\n    let result = repl_loop(&mut session).await;\n\n    result\n}"
    },
    {
      "type": "function_item",
      "name": "display_startup_card",
      "line": 54,
      "column": 0,
      "text": "fn display_startup_card(session: &Session) -> Result<()> {\n    let version = env!(\"CARGO_PKG_VERSION\");\n    println!(\"\\nCodelet v{}\", version);\n\n    let manager = session.provider_manager();\n    if !manager.has_any_provider() {\n        println!(\"Available models: No providers configured\");\n        println!(\"Please set ANTHROPIC_API_KEY, OPENAI_API_KEY, or other credentials\\n\");\n    } else {\n        let providers = manager.list_available_providers();\n        println!(\"Available models: {}\\n\", providers.join(\", \"));\n    }\n\n    Ok(())\n}"
    },
    {
      "type": "function_item",
      "name": "repl_loop",
      "line": 71,
      "column": 0,
      "text": "async fn repl_loop(session: &mut Session) -> Result<()> {\n    let mut input_queue = InputQueue::new();\n    let is_interrupted = Arc::new(AtomicBool::new(false));\n\n    println!(\"Enter your prompt (or 'exit' to quit):\");\n\n    loop {\n        // Read user input with provider-prefixed prompt\n        print!(\"{}\", session.provider_manager().get_prompt_prefix());\n        std::io::stdout().flush()?;\n\n        let mut input = String::new();\n        std::io::stdin().read_line(&mut input)?;\n        let input = input.trim();\n\n        // Check for exit\n        if matches!(input, \"exit\" | \"/quit\" | \"quit\") {\n            println!(\"Goodbye!\");\n            break;\n        }\n\n        // Check for provider switch - CLEARS CONTEXT (CLI-008)\n        if input.starts_with('/') {\n            let provider = input.trim_start_matches('/');\n            match session.switch_provider(provider) {\n                Ok(()) => {\n                    info!(\"Provider switched to: {}\", provider);\n                    println!(\"Switched to {} provider\\n\", provider);\n                    continue;\n                }\n                Err(e) => {\n                    debug!(\"Provider switch failed: {}\", e);\n                    eprintln!(\"Error switching provider: {}\\n\", e);\n                    continue;\n                }\n            }\n        }\n\n        if input.is_empty() {\n            continue;\n        }\n\n        // Run agent with interruption support and persistent context (CLI-008)\n        // Enable raw mode only during agent execution for ESC key detection\n        is_interrupted.store(false, Ordering::Relaxed);\n        enable_raw_mode()?;\n        let mut event_stream = create_event_stream();\n\n        let agent_result = run_agent_with_interruption(\n            session,\n            input,\n            &mut event_stream,\n            &mut input_queue,\n            is_interrupted.clone(),\n        )\n        .await;\n\n        // Always disable raw mode after agent completes\n        disable_raw_mode()?;\n\n        match agent_result {\n            Ok(()) => println!(\"\\n\"),\n            Err(e) => eprintln!(\"Error: {}\\n\", e),\n        }\n    }\n\n    Ok(())\n}"
    },
    {
      "type": "function_item",
      "name": "run_agent_with_interruption",
      "line": 141,
      "column": 0,
      "text": "async fn run_agent_with_interruption(\n    session: &mut Session,\n    prompt: &str,\n    event_stream: &mut (dyn futures::Stream<Item = TuiEvent> + Unpin + Send),\n    input_queue: &mut InputQueue,\n    is_interrupted: Arc<AtomicBool>,\n) -> Result<()> {\n    // Get provider name before mutable borrow (to satisfy borrow checker)\n    let provider_name = session.current_provider_name().to_string();\n    let manager = session.provider_manager_mut();\n\n    // Macro to eliminate code duplication across provider branches (DRY principle)\n    macro_rules! run_with_provider {\n        ($get_provider:ident) => {{\n            let provider = manager.$get_provider()?;\n            let rig_agent = provider.create_rig_agent();\n            let agent = RigAgent::with_default_depth(rig_agent);\n            run_agent_stream_with_interruption(\n                agent,\n                prompt,\n                session, // Pass entire session for token tracking and compaction (CLI-010)\n                event_stream,\n                input_queue,\n                is_interrupted,\n            )\n            .await\n        }};\n    }\n\n    // Dispatch to provider-specific agent\n    match provider_name.as_str() {\n        \"claude\" => run_with_provider!(get_claude),\n        \"openai\" => run_with_provider!(get_openai),\n        \"codex\" => run_with_provider!(get_codex),\n        \"gemini\" => run_with_provider!(get_gemini),\n        _ => Err(anyhow::anyhow!(\"Unknown provider\")),\n    }\n}"
    },
    {
      "type": "function_item",
      "name": "commit_complete_lines",
      "line": 182,
      "column": 0,
      "text": "fn commit_complete_lines(buffer: &str, committed_line_count: &mut usize) -> Vec<String> {\n    commit_complete_lines_ratatui(buffer, committed_line_count, None)\n}"
    },
    {
      "type": "function_item",
      "name": "finalize_and_drain",
      "line": 188,
      "column": 0,
      "text": "fn finalize_and_drain(buffer: &str, committed_line_count: &mut usize) -> Vec<String> {\n    finalize_and_drain_ratatui(buffer, committed_line_count, None)\n}"
    },
    {
      "type": "function_item",
      "name": "add_assistant_text_message",
      "line": 194,
      "column": 0,
      "text": "fn add_assistant_text_message(messages: &mut Vec<rig::message::Message>, text: String) {\n    use rig::message::{AssistantContent, Message, Text};\n    use rig::OneOrMany;\n\n    messages.push(Message::Assistant {\n        id: None,\n        content: OneOrMany::one(AssistantContent::Text(Text { text })),\n    });\n}"
    },
    {
      "type": "function_item",
      "name": "add_assistant_tool_calls_message",
      "line": 206,
      "column": 0,
      "text": "fn add_assistant_tool_calls_message(\n    messages: &mut Vec<rig::message::Message>,\n    tool_calls: Vec<rig::message::AssistantContent>,\n) -> Result<()> {\n    use rig::message::Message;\n    use rig::OneOrMany;\n    use tracing::error;\n\n    match OneOrMany::many(tool_calls) {\n        Ok(content) => {\n            messages.push(Message::Assistant { id: None, content });\n            Ok(())\n        }\n        Err(e) => {\n            error!(\"Failed to convert tool calls to message: {:?}\", e);\n            Err(anyhow::anyhow!(\"Failed to convert tool calls: {:?}\", e))\n        }\n    }\n}"
    },
    {
      "type": "function_item",
      "name": "handle_text_chunk",
      "line": 227,
      "column": 0,
      "text": "fn handle_text_chunk(\n    text: &str,\n    text_buffer: &mut String,\n    assistant_text: &mut String,\n    committed_line_count: &mut usize,\n) -> Result<()> {\n    // Accumulate text (like codex MarkdownStreamCollector.push_delta)\n    text_buffer.push_str(text);\n\n    // CRITICAL: Also accumulate for message history (CLI-008)\n    assistant_text.push_str(text);\n\n    // If chunk contains newline, commit complete lines (like codex)\n    if text.contains('\\n') {\n        debug!(\"Text buffer before commit: {:?}\", text_buffer);\n        let new_lines = commit_complete_lines(text_buffer, committed_line_count);\n        debug!(\"Committed {} new lines\", new_lines.len());\n        for (i, line) in new_lines.iter().enumerate() {\n            debug!(\"Line {}: {:?}\", i, line);\n        }\n\n        // Print new lines\n        for line in &new_lines {\n            print!(\"{}\\r\\n\", line);\n        }\n        std::io::stdout().flush()?;\n    }\n\n    Ok(())\n}"
    },
    {
      "type": "function_item",
      "name": "handle_tool_call",
      "line": 259,
      "column": 0,
      "text": "fn handle_tool_call(\n    tool_call: &rig::message::ToolCall,\n    messages: &mut Vec<rig::message::Message>,\n    text_buffer: &mut String,\n    assistant_text: &mut String,\n    tool_calls_buffer: &mut Vec<rig::message::AssistantContent>,\n    last_tool_name: &mut Option<String>,\n    committed_line_count: &mut usize,\n) -> Result<()> {\n    use rig::message::AssistantContent;\n\n    // Finalize any remaining text before tool call (like codex finalize_and_drain)\n    if !text_buffer.is_empty() {\n        let remaining_lines = finalize_and_drain(text_buffer, committed_line_count);\n        for line in remaining_lines {\n            print!(\"{}\\r\\n\", line);\n        }\n        text_buffer.clear();\n        *committed_line_count = 0;\n    }\n\n    // CRITICAL: Add assistant text to messages if we have any (CLI-008)\n    if !assistant_text.is_empty() {\n        add_assistant_text_message(messages, assistant_text.clone());\n        assistant_text.clear();\n    }\n\n    // Display tool name and arguments before execution\n    let tool_name = &tool_call.function.name;\n    let args = &tool_call.function.arguments;\n\n    // Store tool name for diff rendering (CLI-007)\n    *last_tool_name = Some(tool_name.clone());\n\n    // CRITICAL: Track tool call for message history (CLI-008)\n    tool_calls_buffer.push(AssistantContent::ToolCall(tool_call.clone()));\n\n    // Log full details\n    debug!(\"Tool call: {} with args: {:?}\", tool_name, args);\n\n    // Display tool name\n    print!(\"\\r\\n[Planning to use tool: {}]\", tool_name);\n\n    // Display arguments with bash syntax highlighting for Bash tool (CLI-006)\n    if let Some(obj) = args.as_object() {\n        if !obj.is_empty() {\n            for (key, value) in obj.iter() {\n                // Format value based on type\n                let formatted_value = match value {\n                    serde_json::Value::String(s) => {\n                        // Apply bash highlighting for Bash tool command parameter\n                        if tool_name == \"Bash\" && key == \"command\" {\n                            let highlighted = highlight_bash_command(s);\n                            highlighted.join(\"\\r\\n  \")\n                        } else {\n                            s.clone()\n                        }\n                    }\n                    serde_json::Value::Number(n) => n.to_string(),\n                    serde_json::Value::Bool(b) => b.to_string(),\n                    serde_json::Value::Array(_) => format!(\"{}\", value),\n                    serde_json::Value::Object(_) => format!(\"{}\", value),\n                    serde_json::Value::Null => \"null\".to_string(),\n                };\n                print!(\"\\r\\n  {}: {}\", key, formatted_value);\n            }\n        }\n    }\n    println!(\"\\r\\n\");\n    std::io::stdout().flush()?;\n\n    Ok(())\n}"
    },
    {
      "type": "function_item",
      "name": "handle_tool_result",
      "line": 334,
      "column": 0,
      "text": "fn handle_tool_result(\n    tool_result: &rig::message::ToolResult,\n    messages: &mut Vec<rig::message::Message>,\n    tool_calls_buffer: &mut Vec<rig::message::AssistantContent>,\n    last_tool_name: &Option<String>,\n) -> Result<()> {\n    use rig::message::{Message, ToolResultContent, UserContent};\n    use rig::OneOrMany;\n\n    // CRITICAL: Add buffered tool calls as assistant message (CLI-008)\n    // This happens ONCE before the first tool result\n    if !tool_calls_buffer.is_empty() {\n        add_assistant_tool_calls_message(messages, tool_calls_buffer.clone())?;\n        tool_calls_buffer.clear();\n    }\n\n    // CRITICAL: Add tool result to message history (CLI-008)\n    let tool_result_clone = tool_result.clone();\n    messages.push(Message::User {\n        content: OneOrMany::one(UserContent::ToolResult(tool_result_clone)),\n    });\n\n    // Display tool result with preview (similar to codelet pattern)\n    debug!(\"Tool result received: {:?}\", tool_result);\n\n    // Extract text content from tool result by iterating over OneOrMany\n    let result_parts: Vec<String> = tool_result\n        .content\n        .clone()\n        .into_iter()\n        .map(|content| match content {\n            ToolResultContent::Text(text) => text.text.clone(),\n            ToolResultContent::Image(_) => \"[Image]\".to_string(),\n        })\n        .collect();\n\n    let mut result_text = result_parts.join(\"\\n\");\n\n    // Strip surrounding quotes if present (JSON-escaped string)\n    if result_text.starts_with('\"') && result_text.ends_with('\"') {\n        result_text = result_text[1..result_text.len() - 1].to_string();\n    }\n\n    // Unescape common JSON escape sequences\n    result_text = result_text\n        .replace(\"\\\\n\", \"\\n\")\n        .replace(\"\\\\t\", \"\\t\")\n        .replace(\"\\\\r\", \"\\r\")\n        .replace(\"\\\\\\\"\", \"\\\"\")\n        .replace(\"\\\\\\\\\", \"\\\\\");\n\n    // Apply diff rendering for Edit/Write tools (CLI-007)\n    if let Some(ref tool) = last_tool_name {\n        if tool == \"Edit\" || tool == \"Write\" {\n            // Parse result for file changes and apply diff rendering\n            let lines: Vec<&str> = result_text.lines().collect();\n            let mut formatted_lines = Vec::new();\n\n            for (i, line) in lines.iter().enumerate() {\n                // Detect additions/deletions/context\n                let (prefix, content) = if let Some(stripped) = line.strip_prefix('+') {\n                    ('+', stripped)\n                } else if let Some(stripped) = line.strip_prefix('-') {\n                    ('-', stripped)\n                } else if line.starts_with(\"File:\") {\n                    // Keep file headers as-is\n                    formatted_lines.push(line.to_string());\n                    continue;\n                } else {\n                    (' ', *line)\n                };\n\n                // Apply diff rendering\n                let diff_line = render_diff_line(i + 1, prefix, content);\n                formatted_lines.push(diff_line);\n            }\n\n            result_text = formatted_lines.join(\"\\n\");\n        }\n    }\n\n    // Truncate result if too long (like codelet does at 500 chars)\n    const MAX_PREVIEW_LENGTH: usize = 500;\n    let preview = if result_text.len() > MAX_PREVIEW_LENGTH {\n        format!(\"{}...\", &result_text[..MAX_PREVIEW_LENGTH])\n    } else {\n        result_text\n    };\n\n    // Display with proper formatting for raw mode\n    // Indent each line by 2 spaces and replace \\n with \\r\\n\n    let indented_lines: Vec<String> = preview.lines().map(|line| format!(\"  {}\", line)).collect();\n    let formatted_preview = indented_lines.join(\"\\r\\n\");\n\n    print!(\n        \"\\r\\n[Tool result preview]\\r\\n-------\\r\\n{}\\r\\n-------\\r\\n\",\n        formatted_preview\n    );\n    std::io::stdout().flush()?;\n\n    Ok(())\n}"
    },
    {
      "type": "function_item",
      "name": "handle_final_response",
      "line": 438,
      "column": 0,
      "text": "fn handle_final_response(\n    text_buffer: &str,\n    assistant_text: &str,\n    messages: &mut Vec<rig::message::Message>,\n    committed_line_count: &mut usize,\n) -> Result<()> {\n    // Finalize any remaining text (like codex finalize_and_drain)\n    if !text_buffer.is_empty() {\n        let remaining_lines = finalize_and_drain(text_buffer, committed_line_count);\n        for line in remaining_lines {\n            print!(\"{}\\r\\n\", line);\n        }\n        std::io::stdout().flush()?;\n    }\n\n    // CRITICAL: Add final assistant text to message history (CLI-008)\n    if !assistant_text.is_empty() {\n        add_assistant_text_message(messages, assistant_text.to_owned());\n    }\n\n    Ok(())\n}"
    },
    {
      "type": "function_item",
      "name": "run_agent_stream_with_interruption",
      "line": 464,
      "column": 0,
      "text": "async fn run_agent_stream_with_interruption<M>(\n    agent: RigAgent<M>,\n    prompt: &str,\n    session: &mut Session, // CLI-010: Need full session for token tracking and compaction\n    event_stream: &mut (dyn futures::Stream<Item = TuiEvent> + Unpin + Send),\n    input_queue: &mut InputQueue,\n    is_interrupted: Arc<AtomicBool>,\n) -> Result<()>\nwhere\n    M: CompletionModel,\n{\n    use rig::message::{Message, UserContent};\n    use rig::OneOrMany;\n\n    // CRITICAL: Add user prompt to message history for persistence (CLI-008)\n    session.messages.push(Message::User {\n        content: OneOrMany::one(UserContent::text(prompt)),\n    });\n\n    // CRITICAL FIX: Pass conversation history to rig for context persistence (CLI-008)\n    let mut stream = agent\n        .prompt_streaming_with_history(prompt, &mut session.messages)\n        .await;\n    let status = StatusDisplay::new();\n    let mut status_interval = interval(Duration::from_secs(1));\n\n    // Line-gated markdown streaming (like codex MarkdownStreamCollector)\n    let mut text_buffer = String::new();\n    let mut committed_line_count = 0;\n\n    // Track assistant response content for adding to messages (CLI-008)\n    let mut assistant_text = String::new();\n    let mut tool_calls_buffer: Vec<rig::message::AssistantContent> = Vec::new();\n\n    // Track last tool call for diff rendering (CLI-007)\n    let mut last_tool_name: Option<String> = None;\n\n    // CLI-010: Track token usage for compaction\n    let mut turn_input_tokens: u64 = 0;\n    let mut turn_output_tokens: u64 = 0;\n    let mut turn_cache_read_tokens: Option<u64> = None;\n    let mut turn_cache_creation_tokens: Option<u64> = None;\n\n    loop {\n        tokio::select! {\n            // Agent streaming\n            chunk = stream.next() => {\n                if is_interrupted.load(Ordering::Relaxed) {\n                    // Interrupted - show queued inputs\n                    println!(\"\\n⚠️ Agent interrupted\");\n                    let queued = input_queue.dequeue_all();\n                    if queued.is_empty() {\n                        println!(\"Queued inputs: (none)\");\n                    } else {\n                        println!(\"Queued inputs:\\n{}\", queued.join(\"\\n\\n\"));\n                    }\n                    break;\n                }\n\n                match chunk {\n                    Some(Ok(MultiTurnStreamItem::StreamAssistantItem(StreamedAssistantContent::Text(text)))) => {\n                        handle_text_chunk(\n                            &text.text,\n                            &mut text_buffer,\n                            &mut assistant_text,\n                            &mut committed_line_count,\n                        )?;\n                    }\n                    Some(Ok(MultiTurnStreamItem::StreamAssistantItem(StreamedAssistantContent::ToolCall(tool_call)))) => {\n                        handle_tool_call(\n                            &tool_call,\n                            &mut session.messages,\n                            &mut text_buffer,\n                            &mut assistant_text,\n                            &mut tool_calls_buffer,\n                            &mut last_tool_name,\n                            &mut committed_line_count,\n                        )?;\n                    }\n                    Some(Ok(MultiTurnStreamItem::StreamUserItem(StreamedUserContent::ToolResult(tool_result)))) => {\n                        handle_tool_result(\n                            &tool_result,\n                            &mut session.messages,\n                            &mut tool_calls_buffer,\n                            &last_tool_name,\n                        )?;\n                    }\n                    Some(Ok(MultiTurnStreamItem::FinalResponse(final_resp))) => {\n                        // CLI-010: Extract token usage from FinalResponse\n                        let usage = final_resp.usage();\n\n                        // Use API values if available, otherwise estimate (matches codelet fallback pattern)\n                        turn_input_tokens = if usage.input_tokens > 0 {\n                            usage.input_tokens\n                        } else {\n                            estimate_tokens(prompt)\n                        };\n\n                        turn_output_tokens = if usage.output_tokens > 0 {\n                            usage.output_tokens\n                        } else {\n                            estimate_tokens(&assistant_text)\n                        };\n\n                        // Try to extract cache tokens for Anthropic provider\n                        // NOTE: rig's generic Usage doesn't have cache fields, but we can\n                        // access provider-specific data through the response if needed\n                        // For now, cache tokens remain None for all providers\n                        // TODO: Implement Anthropic-specific cache extraction\n                        turn_cache_read_tokens = None;\n                        turn_cache_creation_tokens = None;\n\n                        handle_final_response(\n                            &text_buffer,\n                            &assistant_text,\n                            &mut session.messages,\n                            &mut committed_line_count,\n                        )?;\n                        // Stream complete\n                        break;\n                    }\n                    Some(Err(e)) => {\n                        return Err(anyhow::anyhow!(\"Agent error: {}\", e));\n                    }\n                    _ => {\n                        // Other stream items\n                    }\n                }\n            }\n\n            // Terminal events\n            event = event_stream.next() => {\n                match event {\n                    Some(TuiEvent::Key(key)) if key.code == KeyCode::Esc => {\n                        is_interrupted.store(true, Ordering::Relaxed);\n                    }\n                    _ => {}\n                }\n            }\n\n            // Status display updates\n            _ = status_interval.tick() => {\n                // Update status (in real implementation, would render to UI)\n                // For now, just track elapsed time\n                let _ = status.format_status();\n            }\n        }\n    }\n\n    // CLI-010: After stream completes, accumulate tokens and check compaction\n    if !is_interrupted.load(Ordering::Relaxed) {\n        // Accumulate tokens into session tracker\n        session.token_tracker.input_tokens += turn_input_tokens;\n        session.token_tracker.output_tokens += turn_output_tokens;\n        if let Some(cache_read) = turn_cache_read_tokens {\n            let current = session.token_tracker.cache_read_input_tokens.unwrap_or(0);\n            session.token_tracker.cache_read_input_tokens = Some(current + cache_read);\n        }\n        if let Some(cache_create) = turn_cache_creation_tokens {\n            let current = session\n                .token_tracker\n                .cache_creation_input_tokens\n                .unwrap_or(0);\n            session.token_tracker.cache_creation_input_tokens = Some(current + cache_create);\n        }\n\n        // Convert messages to ConversationTurn\n        // For simplicity, assume last user message + tool calls + assistant response = 1 turn\n        let turn = create_conversation_turn_from_last_interaction(\n            &session.messages,\n            turn_input_tokens + turn_output_tokens,\n        );\n        if let Some(turn) = turn {\n            session.turns.push(turn);\n        }\n\n        // Check if compaction should trigger\n        const CONTEXT_WINDOW: u64 = 100_000; // 100k tokens\n        let threshold = (CONTEXT_WINDOW as f64 * 0.9) as u64; // 90% threshold\n        let effective = session.token_tracker.effective_tokens();\n\n        if effective > threshold {\n            println!(\"\\n[Generating summary...]\");\n            std::io::stdout().flush()?;\n\n            // Execute compaction\n            match execute_compaction(session).await {\n                Ok(metrics) => {\n                    println!(\n                        \"[Context compacted: {}→{} tokens, {:.0}% compression]\\n\",\n                        metrics.original_tokens,\n                        metrics.compacted_tokens,\n                        metrics.compression_ratio * 100.0\n                    );\n                }\n                Err(e) => {\n                    eprintln!(\"Warning: Compaction failed: {}\", e);\n                }\n            }\n        }\n    }\n\n    Ok(())\n}"
    }
  ]
}
